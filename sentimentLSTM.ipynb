{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4878016af0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import string\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and preprocess data\n",
    "\n",
    "def read_input(input_file):\n",
    "    with zipfile.ZipFile(input_file) as z:\n",
    "        with z.open(input_file[:-4], 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                # skip header\n",
    "                if i == 0: continue\n",
    "                decoded_line = line.decode('utf-8').split('\\t')\n",
    "                # lower case and remove punctuation\n",
    "                line = [l.lower() for l in decoded_line[2] if l not in string.punctuation]\n",
    "                line = ''.join(line).split()\n",
    "                yield (line, int(decoded_line[-1].rstrip('\\n')))\n",
    "\n",
    "def preprocess(input_file):\n",
    "    # get list of data points\n",
    "    data = list(read_input('train.tsv.zip'))\n",
    "    # get rid of 0-length reviews\n",
    "    data = [(review, label) for (review, label) in data if len(review) > 0]\n",
    "    \n",
    "    return data\n",
    "    \n",
    "file = 'train.tsv.zip'\n",
    "data = preprocess(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode reviews\n",
    "\n",
    "all_reviews = [\" \".join(review) for review,label in data]\n",
    "all_labels = np.array([label for review,label in data])\n",
    "all_words = \" \".join(all_reviews).split()\n",
    "all_reviews = [review.split() for review in all_reviews]\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "word_list = word_counts.most_common(len(all_words))\n",
    "word_to_ix = {word:i+1 for i, (word,count) in enumerate(word_list)}\n",
    "ix_to_word = {ix:word for word, ix in word_to_ix.items()}\n",
    "encoded_reviews = [[word_to_ix[word] for word in review] for review in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all reviews the same length\n",
    "\n",
    "def pad_data(encoded_reviews):\n",
    "    max_length = len(max(encoded_reviews, key = len))\n",
    "    reviews = []\n",
    "    for review in encoded_reviews:\n",
    "        if len(review) < max_length:\n",
    "            reviews.append([0]*(max_length - len(review)) + review)\n",
    "        else:\n",
    "            reviews.append(review)\n",
    "    return np.array(reviews)\n",
    "\n",
    "padded_reviews = pad_data(encoded_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training, validation, & test sets\n",
    "\n",
    "train_index = int(len(padded_reviews)*0.7)\n",
    "valid_index = int(len(padded_reviews)*0.85)\n",
    "\n",
    "train_reviews = padded_reviews[:train_index]\n",
    "valid_reviews = padded_reviews[train_index:valid_index]\n",
    "test_reviews = padded_reviews[valid_index:]\n",
    "\n",
    "# get labels for all 3 sets\n",
    "train_labels = all_labels[:train_index]\n",
    "valid_labels = all_labels[train_index:valid_index]\n",
    "test_labels = all_labels[valid_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders\n",
    "\n",
    "train_data = TensorDataset(torch.LongTensor(train_reviews), torch.Tensor(train_labels))\n",
    "valid_data = TensorDataset(torch.LongTensor(valid_reviews), torch.Tensor(valid_labels))\n",
    "test_data = TensorDataset(torch.LongTensor(test_reviews), torch.Tensor(test_labels))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = 50, shuffle = True, drop_last = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = 50, shuffle = True, drop_last = True)\n",
    "test_loader = DataLoader(test_data, batch_size = 50, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "class LSTMSentiment(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size, layers):\n",
    "        super(LSTMSentiment, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.layers = layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, layers, batch_first = True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.word_embeddings(inputs)\n",
    "        lstm_out, h = self.lstm(embeds)\n",
    "        label_space = self.hidden2label(lstm_out)\n",
    "        label_scores = F.log_softmax(label_space, dim=1)\n",
    "        softmax_last = label_scores[:, -1]\n",
    "\n",
    "        return softmax_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/7 Training Loss: 3.8707 Validation Loss: 3.8579 Training Accuracy: 0.222096 Validation Accuracy: 0.219398\n",
      "Epoch: 2/7 Training Loss: 0.1021 Validation Loss: 0.0963 Training Accuracy: 0.516146 Validation Accuracy: 0.498441\n",
      "Epoch: 3/7 Training Loss: 0.0370 Validation Loss: 0.0541 Training Accuracy: 0.517226 Validation Accuracy: 0.501047\n",
      "Epoch: 4/7 Training Loss: 0.0152 Validation Loss: 0.0227 Training Accuracy: 0.518490 Validation Accuracy: 0.500790\n",
      "Epoch: 5/7 Training Loss: 0.0058 Validation Loss: 0.0120 Training Accuracy: 0.517785 Validation Accuracy: 0.496347\n",
      "Epoch: 6/7 Training Loss: 0.0091 Validation Loss: 0.0155 Training Accuracy: 0.517364 Validation Accuracy: 0.496261\n",
      "Epoch: 7/7 Training Loss: 0.0036 Validation Loss: 0.0057 Training Accuracy: 0.517876 Validation Accuracy: 0.497714\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "embedding_dim = 50\n",
    "hidden_dim = 50\n",
    "output_dim = 5\n",
    "layers = 2\n",
    "num_epochs = 7\n",
    "step = 0\n",
    "\n",
    "model = LSTMSentiment(embedding_dim, hidden_dim, output_dim, vocab_size, layers)\n",
    "model.cuda()\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        label_scores = model(inputs)\n",
    "        loss = loss_function(label_scores, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print loss and accuracy for training and validation sets every 2200 steps\n",
    "        if (step % 2200) == 0:            \n",
    "            v_num_correct = 0\n",
    "            valid_losses = []\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = v_inputs.cuda(), v_labels.cuda()\n",
    "\n",
    "                v_output = model(v_inputs)\n",
    "                \n",
    "                # get loss\n",
    "                v_loss = loss_function(v_output, v_labels.long())\n",
    "                valid_losses.append(v_loss.item())\n",
    "                \n",
    "                # get accuracy\n",
    "                v_predictions = [list(pred).index(max(list(pred))) for pred in v_output]\n",
    "                v_correct = [int(p == i) for p, i in zip(v_predictions, list(v_labels))]\n",
    "                v_num_correct += np.sum(v_correct)\n",
    "                \n",
    "            t_num_correct = 0    \n",
    "            for t_inputs, t_labels in train_loader:\n",
    "                t_inputs, t_labels = t_inputs.cuda(), t_labels.cuda()\n",
    "\n",
    "                t_output = model(t_inputs)\n",
    "                \n",
    "                # get accuracy\n",
    "                t_predictions = [list(pred).index(max(list(pred))) for pred in t_output]\n",
    "                t_correct = [int(p == i) for p, i in zip(t_predictions, list(t_labels))]\n",
    "                t_num_correct += np.sum(t_correct)\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), num_epochs),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)),\n",
    "                  \"Training Accuracy: {:.6f}\".format(t_num_correct/len(train_loader.dataset)),\n",
    "                  \"Validation Accuracy: {:.6f}\".format(v_num_correct/len(valid_loader.dataset)))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0098 Test Accuracy: 0.481735\n"
     ]
    }
   ],
   "source": [
    "# test on test set\n",
    "\n",
    "num_correct = 0\n",
    "test_losses = []\n",
    "        \n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    output = model(inputs)\n",
    "                \n",
    "    # get loss\n",
    "    loss = loss_function(output, labels.long())\n",
    "    test_losses.append(loss.item())\n",
    "                \n",
    "    # get accuracy\n",
    "    predictions = [list(pred).index(max(list(pred))) for pred in output]\n",
    "    correct = [int(p == i) for p, i in zip(predictions, list(labels))]\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)),\n",
    "      \"Test Accuracy: {:.6f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
